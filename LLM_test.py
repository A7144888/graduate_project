import pandas as pd
import ollama
import json
import cloudscraper
import requests
from bs4 import BeautifulSoup

# --- è¨­å®šå€ ---
INPUT_FILE = "news.csv"
OUTPUT_FILE = "test_analysis.csv"
MODEL_NAME = "llama3.1:latest"
TEST_COUNT = 3

# åˆå§‹åŒ–çˆ¬èŸ²å™¨
scraper = cloudscraper.create_scraper()

def get_real_url(google_url):
    """ ç›´æ¥è¿½è¹¤ Google News çš„è·³è½‰ï¼Œç²å–çœŸå¯¦ç¶²å€ """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.get(google_url, headers=headers, timeout=10, allow_redirects=True)
        real_url = response.url
        
        if "news.google.com" not in real_url:
            return real_url
        else:
            soup = BeautifulSoup(response.text, 'html.parser')
            target_link = soup.find('a', {'jslog': True})
            if target_link and target_link.get('href'):
                return target_link['href']
    except Exception as e:
        print(f"   âš ï¸ è·³è½‰è¿½è¹¤å¤±æ•—: {e}")
    return google_url

def fetch_news_content_stable(url):
    """ ç²å–çœŸå¯¦ç¶²å€å¾ŒæŠ“å–å…§æ–‡ """
    try:
        real_url = get_real_url(url)
        if "news.google.com" in real_url and len(real_url) < 200:
            return ""
            
        print(f"   ğŸ”— æˆåŠŸè·³è½‰è‡³: {real_url[:60]}...")
        
        response = scraper.get(real_url, timeout=12)
        response.encoding = response.apparent_encoding
        
        if response.status_code != 200:
            return ""

        soup = BeautifulSoup(response.text, 'html.parser')
        for s in soup(["script", "style", "nav", "footer", "header", "aside"]):
            s.extract()

        content_selectors = ['article', '.article-content', '.caas-body', '.story', '.news-content']
        text = ""
        for sel in content_selectors:
            target = soup.select_one(sel)
            if target:
                text = target.get_text(separator=' ', strip=True)
                break
        
        if not text:
            text = " ".join([p.get_text(strip=True) for p in soup.find_all(['p', 'div']) if len(p.get_text(strip=True)) > 25])

        clean_text = " ".join(text.split())
        return clean_text[:2000] if len(clean_text) > 80 else ""

    except Exception as e:
        print(f"   âš ï¸ æŠ“å–ç•°å¸¸: {e}")
        return ""

def analyze_full_news(stock_id, title, content):
    """ é€äº¤ Ollama åˆ†æ  """
    context_text = content if content else "ï¼ˆç„¡å…§æ–‡ï¼Œè«‹åƒ…å°±æ¨™é¡Œåˆ†æï¼‰"
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­å°è‚¡åˆ†æå¸«ã€‚è«‹åˆ†æä»¥ä¸‹æ–°èå°è‚¡ç¥¨ä»£è™Ÿ {stock_id} çš„å½±éŸ¿ã€‚
    
    ã€æ–°èæ¨™é¡Œã€‘ï¼š{title}
    ã€æ–°èå…§æ–‡ã€‘ï¼š{context_text}

    è«‹åš´æ ¼ä»¥ JSON æ ¼å¼å›å‚³ï¼Œä¸è¦æœ‰è§£é‡‹æ–‡å­—ã€‚
    æŒ‡æ¨™å®šç¾©ï¼š
    - sentiment_score: -1.0 (æ¥µå¤§åˆ©ç©º) è‡³ 1.0 (æ¥µå¤§åˆ©å¤š)ï¼Œ0.0 ç‚ºä¸­ç«‹ã€‚
    - impact_intensity: 0.0 è‡³ 1.0 (å½±éŸ¿å¼·åº¦)ã€‚
    - certainty: 0.0 (å‚³è) è‡³ 1.0 (å·²ç™¼ç”Ÿäº‹å¯¦)ã€‚
    - time_horizon: æ ¹æ“šå½±éŸ¿é•·åº¦åƒ…é™å¡«å…¥ï¼š1.0 (çŸ­æœŸ), 0.5 (ä¸­æœŸ), 0.2 (é•·æœŸ)ã€‚

    å›å‚³æ ¼å¼ï¼š
    {{
        "sentiment_score": float,
        "impact_intensity": float,
        "certainty": float,
        "time_horizon": float,
        "summary": "ä¸€å¥è©±é‡é»",
        "evidence": "è«‹å¾å…§æ–‡æ‘˜éŒ„ä¸€æ®µå…·é«”äº‹å¯¦æˆ–æ•¸æ“šï¼Œè‹¥ç„¡å…§æ–‡å‰‡å¡«'æ¨™é¡Œæ¨è«–'",
        "reason": "è©•åˆ†ç†ç”±"
    }}
    """
    try:
        response = ollama.chat(model=MODEL_NAME, messages=[{'role': 'user', 'content': prompt}], format='json')
        return json.loads(response['message']['content'])
    except Exception as e:
        print(f"   âŒ LLM åˆ†æå‡ºéŒ¯: {e}")
        return None

# --- ä¸»ç¨‹å¼åŸ·è¡Œ ---
try:
    df_all = pd.read_csv(INPUT_FILE)
    df_test = df_all.head(TEST_COUNT).copy()
    print(f"ğŸš€ é–‹å§‹æ¸¬è©¦ ...")

    results = []
    for index, row in df_test.iterrows():
        print(f"\nğŸ‘‰ [{index+1}/{TEST_COUNT}] è™•ç†ä¸­: {row['stock_id']} - {row['title'][:15]}...")
        
        full_text = fetch_news_content_stable(row['link'])
        
        if full_text:
            print(f"   ğŸ” æˆåŠŸæŠ“å–å…§æ–‡ï¼š{len(full_text)} å­—")
        else:
            print(f"   âš ï¸ æŠ“å–å¤±æ•— (å­—æ•¸ç‚º 0)")

        analysis = analyze_full_news(row['stock_id'], row['title'], full_text)
        
        if analysis:
            results.append({**row.to_dict(), **analysis})
            print(f"   âœ… åˆ†æå®Œæˆï¼æƒ…ç·’: {analysis.get('sentiment_score')} | æ™‚æ•ˆ: {analysis.get('urgency')}")
        else:
            results.append(row.to_dict())

    pd.DataFrame(results).to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
    print(f"\nâœ¨ ä»»å‹™å®Œæˆï¼çµæœå·²å­˜è‡³ {OUTPUT_FILE}")

except Exception as e:
    print(f"ğŸ’¥ ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")